input {
    kafka {
        bootstrap_servers => "kafka1:19092,kafka2:19092,kafka3:19092"
        # topics => ["access-log"]
        # (추가) post.original 토픽의 카프카 메시지를 그대로 logstash 로 읽어서 ElasticSearch 색인하고 kibana 에서 확인
        topics => ["post.original"]
        group_id => "logstash"
        consumer_threads => 3
        auto_offset_reset => "earliest"
        codec => "json"
        type => "kafka"
    }
}

# filter {
#     grok {
#         match => { "message" => "%{IPORHOST:client_ip} - - \[%{HTTPDATE:logged_at}\] \"%{WORD:http_method} %{URIPATH:request} HTTP/%{NUMBER:http_version}\" %{NUMBER:response_code} %{NUMBER:bytes} %{NUMBER:duration} \"%{DATA:referrer}\" \"%{DATA:user_agent}\"" }
#     }
#     if "_grokparsefailure" in [tags] {
#         drop {}
#     }
#     date {
#         match => [ "logged_at", "dd/MMM/yyyy:HH:mm:ss Z" ]
#         target => "logged_at"
#     }
#     mutate {
#         # message(원본) 필드 삭제 (grok 으로 유의미한 데이터 파싱했기 때문)
#         remove_field => [ "message" ]
#     }
# }

# 파일에 날짜형식 붙게되면, 매일 인덱스를 만들어 줄 수 없기 때문에
# index-template 이라는 것을 만들어서 인덱스 생성
output {
    elasticsearch {
        hosts => ["elasticsearch:9200"]
        # index => "access-log-%{+YYYY-MM-dd}"
        index => "post-%{+YYYY-MM-dd}"
    }
}
